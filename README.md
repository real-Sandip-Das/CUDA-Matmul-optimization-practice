# CUDA-Matmul-optimization-practice
A working implementation of multiplying a 2-bit weight matrix (plus a floating point scale) with another FP16/BF16 matrix with a few optimizations
